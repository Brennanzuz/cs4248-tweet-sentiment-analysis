{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel, TFAutoModel, RobertaTokenizerFast, TFRobertaModel, TFXLNetModel, XLNetTokenizer\n",
    "import tensorflow as tf\n",
    "import tf_keras\n",
    "from tf_keras.preprocessing.text import Tokenizer\n",
    "from tf_keras.preprocessing.sequence import pad_sequences\n",
    "from tf_keras.layers import *\n",
    "from tf_keras.models import Model, Sequential\n",
    "from tf_keras.callbacks import EarlyStopping, LearningRateScheduler, ModelCheckpoint\n",
    "from tf_keras.utils import to_categorical\n",
    "from tf_keras.models import load_model\n",
    "\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from tensorflow.keras.layers import *\n",
    "# from tensorflow.keras.models import Model, Sequential\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, ModelCheckpoint\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "# from keras.models import load_model\n",
    "\n",
    "# Load pre-trained embeddings\n",
    "import gensim.downloader as api\n",
    "glove_vectors = api.load(\"glove-wiki-gigaword-300\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n",
      "Num GPUs Available: 1\n",
      "GPU name: /physical_device:GPU:0\n",
      "GPU details: {'compute_capability': (7, 5), 'device_name': 'NVIDIA GeForce GTX 1650 Ti'}\n",
      "Matrix multiplication result: tf.Tensor(\n",
      "[[22. 28.]\n",
      " [49. 64.]], shape=(2, 2), dtype=float32)\n",
      "Executed on GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1743505968.543756   24454 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2608 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# Check if TensorFlow can see GPU\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# More detailed GPU information\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        print(\"GPU name:\", gpu.name)\n",
    "        print(\"GPU details:\", tf.config.experimental.get_device_details(gpu))\n",
    "else:\n",
    "    print(\"No GPU detected. TensorFlow is running on CPU.\")\n",
    "\n",
    "# Simple test to confirm GPU operation\n",
    "if gpus:\n",
    "    with tf.device('/GPU:0'):\n",
    "        a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "        b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "        c = tf.matmul(a, b)\n",
    "        print(\"Matrix multiplication result:\", c)\n",
    "        print(\"Executed on GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged = pd.read_csv(\"data/train_merged.csv\")\n",
    "train_merged[\"cleaned_text\"] = train_merged[\"cleaned_text\"].fillna(\"\")\n",
    "train_merged = train_merged.fillna(0)\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(train_merged[\"sentiment\"].values)\n",
    "x = train_merged.drop(\"sentiment\", axis = 1)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, stratify=y, random_state=42, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer and model\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# tokenizer_roberta = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
    "# tokenizer_xlnet = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "\n",
    "# Tokenize data\n",
    "def tokenize_data(tokenizer, texts, max_length=128):\n",
    "    # Make sure texts is a list of strings\n",
    "    if not isinstance(texts, list):\n",
    "        texts = list(texts)\n",
    "    \n",
    "    # Check for any non-string entries and convert them\n",
    "    for i, text in enumerate(texts):\n",
    "        if not isinstance(text, str):\n",
    "            texts[i] = str(text)\n",
    "    \n",
    "    encodings = tokenizer(\n",
    "        texts, \n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    return encodings['input_ids'], encodings['attention_mask']\n",
    "\n",
    "# Tokenize train and validation data\n",
    "X_train_inputs, X_train_masks = tokenize_data(tokenizer_bert, x_train[\"cleaned_text\"].tolist())\n",
    "X_val_inputs, X_val_masks = tokenize_data(tokenizer_bert, x_val[\"cleaned_text\"].tolist())\n",
    "\n",
    "# Tf-idf features\n",
    "vectorizer = TfidfVectorizer()\n",
    "# x_train_tfidf = vectorizer.fit_transform(x_train[\"cleaned_text\"]).toarray()\n",
    "# x_val_tfidf = vectorizer.transform(x_val[\"cleaned_text\"]).toarray()\n",
    "\n",
    "# Numerical features\n",
    "x_train_num = x_train.drop([\"cleaned_text\", \"selected_text\"], axis = 1).values\n",
    "x_val_num = x_val.drop([\"cleaned_text\", \"selected_text\"], axis = 1).values\n",
    "\n",
    "# Convert labels to one-hot if needed (depends on your loss function)\n",
    "y_train_tf = tf.convert_to_tensor(y_train, dtype=tf.int32)\n",
    "y_val_tf = tf.convert_to_tensor(y_val, dtype=tf.int32)\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "batch_size = 64\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(((X_train_inputs, X_train_masks, x_train_num), y_train_tf))\n",
    "train_dataset = train_dataset.shuffle(len(y_train)).batch(batch_size)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(((X_val_inputs, X_val_masks, x_val_num), y_val_tf))\n",
    "val_dataset = val_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transformer-based model for sentiment analysis using TensorFlow\n",
    "class TransformerSentimentClassifier(tf_keras.Model):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(TransformerSentimentClassifier, self).__init__()\n",
    "        self.bert = TFAutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.dropout = tf_keras.layers.Dropout(0.1)\n",
    "        self.layer1 = tf_keras.layers.Dense(128, activation=\"relu\")\n",
    "        self.layer2 = tf_keras.layers.Dense(64, activation=\"relu\")\n",
    "        self.layer3 = tf_keras.layers.Dense(64, activation=\"relu\")\n",
    "        self.layer_num1 = tf_keras.layers.Dense(64, activation=\"relu\")\n",
    "        self.classifier = tf_keras.layers.Dense(num_classes, activation=\"softmax\")\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        # Get transformer outputs\n",
    "        ids, masks, num = inputs\n",
    "        outputs = self.bert(input_ids=ids, attention_mask=masks)\n",
    "        \n",
    "        # Use the [CLS] token representation (first token)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        # Linear Layer for every input\n",
    "        layer2 = self.layer2(self.layer1(pooled_output))\n",
    "        layer2_num = self.layer_num1(num)\n",
    "        concatenated_output = tf_keras.layers.Concatenate()([layer2, layer2_num])\n",
    "        final_layer = self.layer3(concatenated_output)\n",
    "\n",
    "        # Apply dropout and the classification layer\n",
    "        x = self.dropout(final_layer, training=training)\n",
    "        logits = self.classifier(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "model = TransformerSentimentClassifier()\n",
    "\n",
    "# Compile the model\n",
    "# Enable mixed precision training\n",
    "from tf_keras.mixed_precision import Policy, set_global_policy\n",
    "policy = Policy('mixed_float16')\n",
    "set_global_policy(policy)\n",
    "\n",
    "optimizer = tf_keras.optimizers.Adam(learning_rate=2e-5)\n",
    "loss = tf_keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Training parameters\n",
    "epochs = 3\n",
    "steps_per_epoch = len(train_dataset)\n",
    "total_steps = steps_per_epoch * epochs\n",
    "warmup_steps = int(0.1 * total_steps)  # 10% warmup\n",
    "\n",
    "# Callbacks\n",
    "lr_scheduler = tf_keras.callbacks.LearningRateScheduler(\n",
    "    lambda epoch: optimizer.learning_rate * (epoch / warmup_steps) if epoch < warmup_steps else optimizer.learning_rate\n",
    ")\n",
    "early_stopping = tf_keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[lr_scheduler, early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model after training\n",
    "# 1. Save the entire model (including optimizer state)\n",
    "model.save_weights('all_at_once_weights.h5')\n",
    "\n",
    "# 2. Save the model architecture as JSON (optional)\n",
    "model_json = model.to_json()\n",
    "with open(\"all_at_once.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "print(\"Model saved successfully\")\n",
    "\n",
    "# Later, to load the model:\n",
    "def load_trained_model(model_name, num_classes=3):\n",
    "    # Recreate the model architecture\n",
    "    loaded_model = TransformerSentimentClassifier(model_name=model_name, num_classes=num_classes)\n",
    "    \n",
    "    # Compile the model to initialize weights\n",
    "    loaded_model.compile(\n",
    "        optimizer=tf_keras.optimizers.Adam(),\n",
    "        loss=tf_keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Build the model with a sample input\n",
    "    sample_input_ids = tf.ones((1, 128), dtype=tf.int32)\n",
    "    sample_masks = tf.ones((1, 128), dtype=tf.int32)\n",
    "    _ = loaded_model([sample_input_ids, sample_masks])\n",
    "    \n",
    "    # Load the weights\n",
    "    loaded_model.load_weights('all_at_once_weights.h5')\n",
    "    return loaded_model\n",
    "\n",
    "# Example of loading the model\n",
    "# loaded_model = load_trained_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading in TensorFlow format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model in SavedModel format\n",
    "model.save('all_at_once', save_format='tf')\n",
    "\n",
    "# Later, to load:\n",
    "# loaded_model = tf_keras.models.load_model('transformer_sentiment_model_saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set size: 3534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test data: 100%|██████████| 3534/3534 [00:00<00:00, 26214.82it/s]\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv(\"data/test_merged.csv\").dropna()\n",
    "y_test = label_encoder.transform(test[\"sentiment\"].values)\n",
    "x_test = test.drop(\"sentiment\", axis = 1)\n",
    "X_test_inputs, X_test_masks = tokenize_data(tokenizer_bert, x_test[\"cleaned_text\"].tolist())\n",
    "# x_test_tfidf = vectorizer.transform(x_test[\"cleaned_text\"])\n",
    "x_test_num = x_test.drop([\"cleaned_text\"], axis = 1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "textID",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sentiment",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Time of Tweet",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Age of User",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Country",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Population -2020",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Land Area (Km²)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Density (P/Km²)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "expanded_text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "clean_text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text_without_stopwords",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "tokens",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "lemmatized_tokens",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "lemmatized_sentence",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "label",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "predicted_sentiment",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "f9a7b6f5-8af1-4caa-bce0-85b2899b6720",
       "rows": [
        [
         "0",
         "f87dea47db",
         "Last session of the day  http://twitpic.com/67ezh",
         "neutral",
         "morning",
         "0-20",
         "Afghanistan",
         "38928346.0",
         "652860.0",
         "60.0",
         "Last session of the day  http://twitpic.com/67ezh",
         "last session of the day httptwitpiccom ezh",
         "last session day httptwitpiccom ezh",
         "['last', 'session', 'of', 'the', 'day', 'httptwitpiccom', 'ezh']",
         "['last', 'session', 'of', 'the', 'day', 'httptwitpiccom', 'ezh']",
         "last session of the day httptwitpiccom ezh",
         "1",
         "neutral"
        ],
        [
         "1",
         "96d74cb729",
         " Shanghai is also really exciting (precisely -- skyscrapers galore). Good tweeps in China:  (SH)  (BJ).",
         "positive",
         "noon",
         "21-30",
         "Albania",
         "2877797.0",
         "27400.0",
         "105.0",
         " Shanghai is also really exciting (precisely -- skyscrapers galore). Good tweeps in China:  (SH)  (BJ).",
         "shanghai is also really exciting precisely skyscrapers galore good tweeps in china sh bj",
         "shanghai also really exciting precisely skyscrapers galore good tweeps china sh bj",
         "['shanghai', 'is', 'also', 'really', 'exciting', 'precisely', 'skyscrapers', 'galore', 'good', 'tweeps', 'in', 'china', 'sh', 'bj']",
         "['shanghai', 'be', 'also', 'really', 'excite', 'precisely', 'skyscraper', 'galore', 'good', 'tweeps', 'in', 'china', 'sh', 'bj']",
         "shanghai be also really excite precisely skyscraper galore good tweeps in china sh bj",
         "2",
         "positive"
        ],
        [
         "2",
         "eee518ae67",
         "Recession hit Veronique Branquinho, she has to quit her company, such a shame!",
         "negative",
         "night",
         "31-45",
         "Algeria",
         "43851044.0",
         "2381740.0",
         "18.0",
         "Recession hit Veronique Branquinho, she has to quit her company, such a shame!",
         "recession hit veronique branquinho she has to quit her company such a shame",
         "recession hit veronique branquinho quit company shame",
         "['recession', 'hit', 'veronique', 'branquinho', 'she', 'has', 'to', 'quit', 'her', 'company', 'such', 'a', 'shame']",
         "['recession', 'hit', 'veronique', 'branquinho', 'she', 'have', 'to', 'quit', 'her', 'company', 'such', 'a', 'shame']",
         "recession hit veronique branquinho she have to quit her company such a shame",
         "0",
         "negative"
        ],
        [
         "3",
         "01082688c6",
         " happy bday!",
         "positive",
         "morning",
         "46-60",
         "Andorra",
         "77265.0",
         "470.0",
         "164.0",
         " happy birthday!",
         "happy birthday",
         "happy birthday",
         "['happy', 'birthday']",
         "['happy', 'birthday']",
         "happy birthday",
         "2",
         "positive"
        ],
        [
         "4",
         "33987a8ee5",
         " http://twitpic.com/4w75p - I like it!!",
         "positive",
         "noon",
         "60-70",
         "Angola",
         "32866272.0",
         "1246700.0",
         "26.0",
         " http://twitpic.com/4w75p - I like it!!",
         "httptwitpiccom w p i like it",
         "httptwitpiccom w p like",
         "['httptwitpiccom', 'w', 'p', 'i', 'like', 'it']",
         "['httptwitpiccom', 'w', 'p', 'i', 'like', 'it']",
         "httptwitpiccom w p i like it",
         "2",
         "neutral"
        ]
       ],
       "shape": {
        "columns": 17,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Time of Tweet</th>\n",
       "      <th>Age of User</th>\n",
       "      <th>Country</th>\n",
       "      <th>Population -2020</th>\n",
       "      <th>Land Area (Km²)</th>\n",
       "      <th>Density (P/Km²)</th>\n",
       "      <th>expanded_text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>text_without_stopwords</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "      <th>lemmatized_sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>predicted_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "      <td>neutral</td>\n",
       "      <td>morning</td>\n",
       "      <td>0-20</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>38928346.0</td>\n",
       "      <td>652860.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "      <td>last session of the day httptwitpiccom ezh</td>\n",
       "      <td>last session day httptwitpiccom ezh</td>\n",
       "      <td>['last', 'session', 'of', 'the', 'day', 'httpt...</td>\n",
       "      <td>['last', 'session', 'of', 'the', 'day', 'httpt...</td>\n",
       "      <td>last session of the day httptwitpiccom ezh</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td>Shanghai is also really exciting (precisely -...</td>\n",
       "      <td>positive</td>\n",
       "      <td>noon</td>\n",
       "      <td>21-30</td>\n",
       "      <td>Albania</td>\n",
       "      <td>2877797.0</td>\n",
       "      <td>27400.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>Shanghai is also really exciting (precisely -...</td>\n",
       "      <td>shanghai is also really exciting precisely sky...</td>\n",
       "      <td>shanghai also really exciting precisely skyscr...</td>\n",
       "      <td>['shanghai', 'is', 'also', 'really', 'exciting...</td>\n",
       "      <td>['shanghai', 'be', 'also', 'really', 'excite',...</td>\n",
       "      <td>shanghai be also really excite precisely skysc...</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eee518ae67</td>\n",
       "      <td>Recession hit Veronique Branquinho, she has to...</td>\n",
       "      <td>negative</td>\n",
       "      <td>night</td>\n",
       "      <td>31-45</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>43851044.0</td>\n",
       "      <td>2381740.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Recession hit Veronique Branquinho, she has to...</td>\n",
       "      <td>recession hit veronique branquinho she has to ...</td>\n",
       "      <td>recession hit veronique branquinho quit compan...</td>\n",
       "      <td>['recession', 'hit', 'veronique', 'branquinho'...</td>\n",
       "      <td>['recession', 'hit', 'veronique', 'branquinho'...</td>\n",
       "      <td>recession hit veronique branquinho she have to...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01082688c6</td>\n",
       "      <td>happy bday!</td>\n",
       "      <td>positive</td>\n",
       "      <td>morning</td>\n",
       "      <td>46-60</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>77265.0</td>\n",
       "      <td>470.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>happy birthday!</td>\n",
       "      <td>happy birthday</td>\n",
       "      <td>happy birthday</td>\n",
       "      <td>['happy', 'birthday']</td>\n",
       "      <td>['happy', 'birthday']</td>\n",
       "      <td>happy birthday</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33987a8ee5</td>\n",
       "      <td>http://twitpic.com/4w75p - I like it!!</td>\n",
       "      <td>positive</td>\n",
       "      <td>noon</td>\n",
       "      <td>60-70</td>\n",
       "      <td>Angola</td>\n",
       "      <td>32866272.0</td>\n",
       "      <td>1246700.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>http://twitpic.com/4w75p - I like it!!</td>\n",
       "      <td>httptwitpiccom w p i like it</td>\n",
       "      <td>httptwitpiccom w p like</td>\n",
       "      <td>['httptwitpiccom', 'w', 'p', 'i', 'like', 'it']</td>\n",
       "      <td>['httptwitpiccom', 'w', 'p', 'i', 'like', 'it']</td>\n",
       "      <td>httptwitpiccom w p i like it</td>\n",
       "      <td>2</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text sentiment  \\\n",
       "0  f87dea47db  Last session of the day  http://twitpic.com/67ezh   neutral   \n",
       "1  96d74cb729   Shanghai is also really exciting (precisely -...  positive   \n",
       "2  eee518ae67  Recession hit Veronique Branquinho, she has to...  negative   \n",
       "3  01082688c6                                        happy bday!  positive   \n",
       "4  33987a8ee5             http://twitpic.com/4w75p - I like it!!  positive   \n",
       "\n",
       "  Time of Tweet Age of User      Country  Population -2020  Land Area (Km²)  \\\n",
       "0       morning        0-20  Afghanistan        38928346.0         652860.0   \n",
       "1          noon       21-30      Albania         2877797.0          27400.0   \n",
       "2         night       31-45      Algeria        43851044.0        2381740.0   \n",
       "3       morning       46-60      Andorra           77265.0            470.0   \n",
       "4          noon       60-70       Angola        32866272.0        1246700.0   \n",
       "\n",
       "   Density (P/Km²)                                      expanded_text  \\\n",
       "0             60.0  Last session of the day  http://twitpic.com/67ezh   \n",
       "1            105.0   Shanghai is also really exciting (precisely -...   \n",
       "2             18.0  Recession hit Veronique Branquinho, she has to...   \n",
       "3            164.0                                    happy birthday!   \n",
       "4             26.0             http://twitpic.com/4w75p - I like it!!   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0         last session of the day httptwitpiccom ezh   \n",
       "1  shanghai is also really exciting precisely sky...   \n",
       "2  recession hit veronique branquinho she has to ...   \n",
       "3                                     happy birthday   \n",
       "4                       httptwitpiccom w p i like it   \n",
       "\n",
       "                              text_without_stopwords  \\\n",
       "0                last session day httptwitpiccom ezh   \n",
       "1  shanghai also really exciting precisely skyscr...   \n",
       "2  recession hit veronique branquinho quit compan...   \n",
       "3                                     happy birthday   \n",
       "4                            httptwitpiccom w p like   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  ['last', 'session', 'of', 'the', 'day', 'httpt...   \n",
       "1  ['shanghai', 'is', 'also', 'really', 'exciting...   \n",
       "2  ['recession', 'hit', 'veronique', 'branquinho'...   \n",
       "3                              ['happy', 'birthday']   \n",
       "4    ['httptwitpiccom', 'w', 'p', 'i', 'like', 'it']   \n",
       "\n",
       "                                   lemmatized_tokens  \\\n",
       "0  ['last', 'session', 'of', 'the', 'day', 'httpt...   \n",
       "1  ['shanghai', 'be', 'also', 'really', 'excite',...   \n",
       "2  ['recession', 'hit', 'veronique', 'branquinho'...   \n",
       "3                              ['happy', 'birthday']   \n",
       "4    ['httptwitpiccom', 'w', 'p', 'i', 'like', 'it']   \n",
       "\n",
       "                                 lemmatized_sentence  label  \\\n",
       "0         last session of the day httptwitpiccom ezh      1   \n",
       "1  shanghai be also really excite precisely skysc...      2   \n",
       "2  recession hit veronique branquinho she have to...      0   \n",
       "3                                     happy birthday      2   \n",
       "4                       httptwitpiccom w p i like it      2   \n",
       "\n",
       "  predicted_sentiment  \n",
       "0             neutral  \n",
       "1            positive  \n",
       "2            negative  \n",
       "3            positive  \n",
       "4             neutral  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_label_map = {\n",
    "    0: \"negative\",\n",
    "    1: \"neutral\",\n",
    "    2: \"positive\"\n",
    "}\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict((X_test_inputs, X_test_masks, x_test_num))\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "mapped_predictions = np.array([reverse_label_map[prediction] for prediction in predictions])\n",
    "\n",
    "test[\"predicted_sentiment\"] = mapped_predictions\n",
    "test.to_csv(\"data/test_predictions.csv\", index=False)\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test[\"sentiment\"], test[\"predicted_sentiment\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
