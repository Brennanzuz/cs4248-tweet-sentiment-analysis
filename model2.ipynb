{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "stop_words = stopwords.words('english')\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import gc\n",
    "import ast\n",
    "from string import punctuation\n",
    "import string\n",
    "gc.collect()\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "import emoji\n",
    "from textattack.augmentation import EasyDataAugmenter\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import kagglehub\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import RobertaTokenizerFast, TFRobertaModel\n",
    "from transformers import TFXLNetModel, XLNetTokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.9), please consider upgrading to the latest version (0.3.10).\n"
     ]
    }
   ],
   "source": [
    "path = kagglehub.dataset_download('abhi8923shriv/sentiment-analysis-dataset')\n",
    "train_dataset = path+'/train.csv'\n",
    "test_dataset = path+'/test.csv'\n",
    "train_df = pd.read_csv(train_dataset, encoding='ISO-8859-1')\n",
    "test_df = pd.read_csv(test_dataset, encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abbreviation_replacement(text):\n",
    "  text = re.sub(r\"i\\'m\", \"i am\", text)\n",
    "  text = re.sub(r\"\\'re\", \"are\", text)\n",
    "  text = re.sub(r\"he\\'s\", \"he is\", text)\n",
    "  text = re.sub(r\"it\\'s\", \"it is\", text)\n",
    "  text = re.sub(r\"that\\'s\", \"that is\", text)\n",
    "  text = re.sub(r\"who\\'s\", \"who is\", text)\n",
    "  text = re.sub(r\"what\\'s\", \"what is\", text)\n",
    "  text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "  text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "  text = re.sub(r\"\\'d\", \" would\", text)\n",
    "  text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "  text = re.sub(r\",\", \" , \", text)\n",
    "  text = re.sub(r\"!\", \" ! \", text)\n",
    "  text = re.sub(r\"\\.\", \" \\. \", text)\n",
    "  text = re.sub(r\"\\(\", \" \\( \", text)\n",
    "  text = re.sub(r\"\\)\", \" \\) \", text)\n",
    "  text = re.sub(r\"\\?\", \" \\? \", text)\n",
    "  return text\n",
    "\n",
    "# handle emojis, source: https://github.com/Carmezim/crypto-twitter-sentiment-analysis/blob/master/preprocess.py\n",
    "def handle_emojis(tweet):\n",
    "    # Smile -- :), : ), :-), (:, ( :, (-:, :')\n",
    "    tweet = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))', ' pos_emo ', tweet)\n",
    "    # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
    "    tweet = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' pos_emo ', tweet)\n",
    "    # Love -- <3, :*, ♥, ^^, ^_^\n",
    "    tweet = re.sub(r'(<3|:\\*|♥|\\^\\^|\\^_\\^)', ' pos_emo ', tweet)\n",
    "    # Wink -- ;-), ;), ;-D, ;D, (;,  (-;\n",
    "    tweet = re.sub(r'(;-?\\)|;-?D|\\(-?;)', ' pos_emo ', tweet)\n",
    "    # Sad -- :-(, : (, :(, ):, )-:\n",
    "    tweet = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' neg_emo ', tweet)\n",
    "    # Cry -- :,(, :'(, :\"(, ;(, );\n",
    "    tweet = re.sub(r'(:,\\(|:\\'\\(|:\"\\(|;\\(|\\(\\;)', ' neg_emo ', tweet)\n",
    "    return tweet\n",
    "\n",
    "puntation_dict = str.maketrans('', '', string.punctuation)\n",
    "def drop_punctuation(text):\n",
    "  text = re.sub(r'(.)\\1+', r'\\1\\1', text)\n",
    "  text = text.translate(puntation_dict)\n",
    "  return text\n",
    "\n",
    "def remove_number(text):\n",
    "  text = re.sub(r'(0|1|2|3|4|5|6|7|8|9)', '', text)\n",
    "  return text\n",
    "\n",
    "stoplists = [stoplist for stoplist in stopwords.words('english') if len(stoplist) <= 1]\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def spacy_fast_lemmatizier_stopworder(text, return_tokens=False):\n",
    "  doc = nlp(text)\n",
    "  tokens = [token.lemma_ for token in doc if token.text not in stoplists]\n",
    "\n",
    "  if return_tokens:\n",
    "      return tokens\n",
    "\n",
    "  text = ' '.join(tokens)\n",
    "  return text\n",
    "\n",
    "def preprocessing_all_method(tweet):\n",
    "  tweet = tweet.strip().lower()\n",
    "  tweet = abbreviation_replacement(tweet)\n",
    "  tweet = handle_emojis(tweet)\n",
    "  tweet = drop_punctuation(tweet)\n",
    "  tweet = remove_number(tweet)\n",
    "  tweet = spacy_fast_lemmatizier_stopworder(tweet, return_tokens=False)\n",
    "  return tweet\n",
    "\n",
    "def preprocess(texts):\n",
    "  for i in tqdm(range(len(texts))):\n",
    "      texts[i] = preprocessing_all_method(texts[i])\n",
    "  return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_length(data):\n",
    "  data[\"text_length\"] = data[\"text\"].str.len()\n",
    "  data[\"num_words\"] = data[\"text\"].str.split(\" \").str.len()\n",
    "  return data\n",
    "\n",
    "def get_num_profanity(text):\n",
    "  with open(\"en.txt\", 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    profanity_list = [line.strip() for line in lines]\n",
    "  count = 0\n",
    "  for word in text.split():\n",
    "    if word in profanity_list:\n",
    "      count += 1\n",
    "  return count\n",
    "\n",
    "def get_capital_percentage(text):\n",
    "  capital_letters = sum(1 for char in text if 'A' <= char <= 'Z')\n",
    "  total_letters = sum(1 for char in text if char.isalpha())\n",
    "  if total_letters == 0:\n",
    "     return 0\n",
    "  return capital_letters / total_letters\n",
    "\n",
    "def get_polarity_subjectivity(text):\n",
    "  blob = TextBlob(text)\n",
    "  return blob.sentiment.polarity, blob.sentiment.subjectivity\n",
    "\n",
    "def get_pos_counts(text):\n",
    "  doc = nlp(text)\n",
    "  pos_counts = {\n",
    "      'num_nouns': 0,\n",
    "      'num_verbs': 0,\n",
    "      'num_adjs': 0,\n",
    "      'num_advs': 0\n",
    "  }\n",
    "  for token in doc:\n",
    "      if token.pos_ == 'NOUN':\n",
    "          pos_counts['num_nouns'] += 1\n",
    "      elif token.pos_ == 'VERB':\n",
    "          pos_counts['num_verbs'] += 1\n",
    "      elif token.pos_ == 'ADJ':\n",
    "          pos_counts['num_adjs'] += 1\n",
    "      elif token.pos_ == 'ADV':\n",
    "          pos_counts['num_advs'] += 1\n",
    "  return pos_counts\n",
    "\n",
    "def count_punctuations(text):\n",
    "  return text.count('!'), text.count('?'), text.count('...')\n",
    "\n",
    "def count_uppercase_words(text):\n",
    "  return sum(1 for word in text.split() if word.isupper())\n",
    "\n",
    "def count_negations(text):\n",
    "  negation_words = ['no', 'not', 'never', \"n't\"]\n",
    "  return sum(1 for word in text.lower().split() if word in negation_words)\n",
    "\n",
    "def count_emojis(text):\n",
    "  return emoji.emoji_count(text)\n",
    "\n",
    "def extract_features(df):\n",
    "  df = get_text_length(df)\n",
    "  features = {\n",
    "      \"num_profanity\" : [],\n",
    "      \"capital_percentage\" : [],\n",
    "      'polarity' : [],\n",
    "      'subjectivity' : [],\n",
    "      'num_nouns' : [],\n",
    "      'num_verbs' : [],\n",
    "      'num_adjs' : [],\n",
    "      'num_advs' : [],\n",
    "      'num_exclamations' : [],\n",
    "      'num_questions' : [],\n",
    "      'num_ellipsis' : [],\n",
    "      'uppercase_count' : [],\n",
    "      'negation_count' : [],\n",
    "      'emoji_count' : []\n",
    "  }\n",
    "  for text in tqdm(df['text']):\n",
    "    features[\"num_profanity\"  ].append(get_num_profanity(text))\n",
    "    features[\"capital_percentage\"].append(get_capital_percentage(text))\n",
    "    polarity, subjectivity = get_polarity_subjectivity(text)\n",
    "    features['polarity'].append(polarity)\n",
    "    features['subjectivity'].append(subjectivity)\n",
    "    pos_counts = get_pos_counts(text)\n",
    "    features['num_nouns'].append(pos_counts['num_nouns'])\n",
    "    features['num_verbs'].append(pos_counts['num_verbs'])\n",
    "    features['num_adjs'].append(pos_counts['num_adjs'])\n",
    "    features['num_advs'].append(pos_counts['num_advs'])\n",
    "    exclamations, questions, ellipsis = count_punctuations(text)\n",
    "    features['num_exclamations'].append(exclamations)\n",
    "    features['num_questions'].append(questions)\n",
    "    features['num_ellipsis'].append(ellipsis)\n",
    "    uppercase_count = count_uppercase_words(text)\n",
    "    features['uppercase_count'].append(uppercase_count)\n",
    "    negation_count = count_negations(text)\n",
    "    features['negation_count'].append(negation_count)\n",
    "    emoji_count = count_emojis(text)\n",
    "    features['emoji_count'].append(emoji_count)\n",
    "\n",
    "  feature_df = pd.DataFrame(features)\n",
    "\n",
    "  return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27481/27481 [04:02<00:00, 113.42it/s]\n",
      "100%|██████████| 27481/27481 [03:08<00:00, 145.49it/s]\n",
      "100%|██████████| 4815/4815 [00:29<00:00, 164.54it/s] \n",
      "100%|██████████| 4815/4815 [00:23<00:00, 208.49it/s]\n"
     ]
    }
   ],
   "source": [
    "features_train = extract_features(train_df.fillna(\"\"))\n",
    "train_df[\"cleaned_text\"] = preprocess(train_df[\"text\"].fillna(\"\"))\n",
    "train_df[\"text_length\"] = train_df[\"text\"].str.len()\n",
    "train_df[\"num_words\"] = train_df[\"text\"].str.split(\" \").str.len()\n",
    "train_merged = pd.concat([train_df[[\"cleaned_text\", \"text_length\", \"num_words\", \"sentiment\"]], features_train], axis = 1)\n",
    "\n",
    "features_test = extract_features(test_df.fillna(\"\"))\n",
    "test_df[\"cleaned_text\"] = preprocess(test_df[\"text\"].fillna(\"\"))\n",
    "test_df[\"text_length\"] = test_df[\"text\"].str.len()\n",
    "test_df[\"num_words\"] = test_df[\"text\"].str.split(\" \").str.len()\n",
    "test_merged = pd.concat([test_df[[\"cleaned_text\", \"text_length\", \"num_words\", \"sentiment\"]], features_test], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged.to_csv(\"train_merged.csv\", index = False)\n",
    "test_merged.to_csv(\"test_merged.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged = pd.read_csv(\"train_merged.csv\")\n",
    "train_merged[\"cleaned_text\"] = train_merged[\"cleaned_text\"].fillna(\"\")\n",
    "train_merged = train_merged.fillna(0)\n",
    "test_merged = pd.read_csv(\"test_merged.csv\").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(train_merged[\"sentiment\"].values)\n",
    "x = train_merged.drop(\"sentiment\", axis = 1)\n",
    "y_test = label_encoder.transform(test_merged[\"sentiment\"].values)\n",
    "x_test = test_merged.drop(\"sentiment\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(x, y, stratify=y, random_state=42, test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=100000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(x_train[\"cleaned_text\"])\n",
    "x_train_seq = tokenizer.texts_to_sequences(x_train[\"cleaned_text\"])\n",
    "x_valid_seq = tokenizer.texts_to_sequences(x_valid[\"cleaned_text\"])\n",
    "x_test_seq = tokenizer.texts_to_sequences(x_test[\"cleaned_text\"])\n",
    "max_length = 200\n",
    "x_train_pad = pad_sequences(x_train_seq, maxlen=max_length, padding='post')\n",
    "x_valid_pad = pad_sequences(x_valid_seq, maxlen=max_length, padding='post')\n",
    "x_test_pad = pad_sequences(x_test_seq, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "\n",
    "embedding_dim = 300\n",
    "word_index = tokenizer.word_index\n",
    "num_words = min(100000, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    if word in word2vec:\n",
    "        embedding_matrix[i] = word2vec[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_num = x_train.drop(\"cleaned_text\", axis = 1).values\n",
    "x_valid_num = x_valid.drop(\"cleaned_text\", axis = 1).values\n",
    "x_test_num = x_test.drop(\"cleaned_text\", axis = 1).values\n",
    "y_train_cat = to_categorical(y_train, num_classes=3)\n",
    "y_valid_cat = to_categorical(y_valid, num_classes=3)\n",
    "y_test_cat = to_categorical(y_test, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp_word2vec(x_train_pad, x_valid_pad, x_train_num, x_valid_num, y_train, y_valid):\n",
    "    input_ts = Input(shape=(max_length,))\n",
    "    input_exog = Input(shape=(x_train_num.shape[1], ))\n",
    "\n",
    "    embedding_layer = Embedding(input_dim=num_words,\n",
    "                                output_dim=embedding_dim,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_length,\n",
    "                                trainable=False)(input_ts)\n",
    "\n",
    "    pooled_output = Flatten()(embedding_layer)\n",
    "    layer_1 = Dense(256, kernel_regularizer = tf.keras.regularizers.l2(0.01), activation = \"relu\")(pooled_output)\n",
    "    dropout_1 = Dropout(0.2)(layer_1)\n",
    "    layer_2 = Dense(128, activation = \"relu\")(dropout_1)\n",
    "\n",
    "    layer_num1 = Dense(256, kernel_regularizer = tf.keras.regularizers.l2(0.01), activation = \"relu\")(input_exog)\n",
    "    dropout_2 = Dropout(0.2)(layer_num1)\n",
    "    layer_num2 = Dense(128, activation = \"relu\")(dropout_2)\n",
    "\n",
    "    concat_ft = Concatenate()([layer_2, layer_num2])\n",
    "\n",
    "    prediction = Dense(3, activation = \"softmax\")(concat_ft)\n",
    "    model = Model(inputs=[input_ts, input_exog], outputs=prediction)\n",
    "    lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda x: 1e-4 * math.exp(-0.1 * x))\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                    optimizer=\"adam\",\n",
    "                    metrics=[\"accuracy\"])\n",
    "\n",
    "    history = model.fit([x_train_pad, x_train_num], y_train,\n",
    "                          epochs=20,\n",
    "                          validation_data=([x_valid_pad, x_valid_num], y_valid),\n",
    "                          batch_size=64,\n",
    "                          verbose=1,\n",
    "                          callbacks=[lr_scheduler, early_stopping])\n",
    "\n",
    "    # Save the trained model\n",
    "    model.save('mlp_word2vec.keras')\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 200ms/step - accuracy: 0.3914 - loss: nan - val_accuracy: 0.6461 - val_loss: nan - learning_rate: 1.0000e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 192ms/step - accuracy: 0.6731 - loss: nan - val_accuracy: 0.6650 - val_loss: nan - learning_rate: 9.0484e-05\n",
      "Epoch 3/20\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 192ms/step - accuracy: 0.7005 - loss: nan - val_accuracy: 0.6672 - val_loss: nan - learning_rate: 8.1873e-05\n",
      "Epoch 4/20\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 193ms/step - accuracy: 0.7223 - loss: nan - val_accuracy: 0.6642 - val_loss: nan - learning_rate: 7.4082e-05\n",
      "Epoch 5/20\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 193ms/step - accuracy: 0.7442 - loss: nan - val_accuracy: 0.6682 - val_loss: nan - learning_rate: 6.7032e-05\n",
      "Epoch 6/20\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 192ms/step - accuracy: 0.7622 - loss: nan - val_accuracy: 0.6686 - val_loss: nan - learning_rate: 6.0653e-05\n",
      "Epoch 7/20\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 193ms/step - accuracy: 0.7758 - loss: nan - val_accuracy: 0.6591 - val_loss: nan - learning_rate: 5.4881e-05\n",
      "Epoch 8/20\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 196ms/step - accuracy: 0.7931 - loss: nan - val_accuracy: 0.6668 - val_loss: nan - learning_rate: 4.9659e-05\n",
      "Epoch 9/20\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 204ms/step - accuracy: 0.8116 - loss: nan - val_accuracy: 0.6650 - val_loss: nan - learning_rate: 4.4933e-05\n",
      "Epoch 10/20\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 199ms/step - accuracy: 0.8360 - loss: nan - val_accuracy: 0.6581 - val_loss: nan - learning_rate: 4.0657e-05\n",
      "Epoch 11/20\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 197ms/step - accuracy: 0.8429 - loss: nan - val_accuracy: 0.6566 - val_loss: nan - learning_rate: 3.6788e-05\n"
     ]
    }
   ],
   "source": [
    "model, history = train_mlp_word2vec(x_train_pad, x_valid_pad, x_train_num, x_valid_num, y_train_cat, y_valid_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.59      0.63      1001\n",
      "           1       0.61      0.72      0.66      1430\n",
      "           2       0.74      0.66      0.70      1103\n",
      "\n",
      "    accuracy                           0.66      3534\n",
      "   macro avg       0.68      0.66      0.66      3534\n",
      "weighted avg       0.67      0.66      0.66      3534\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict([x_test_pad, x_test_num])\n",
    "preds = np.argmax(preds, axis=1)\n",
    "label = np.argmax(y_test_cat, axis=1)\n",
    "print(classification_report(label, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp_word2vec_tfidf(x_train_pad, x_valid_pad, x_train_num, x_valid_num, x_train_tfidf, x_valid_tfidf, y_train, y_valid):\n",
    "    input_ts = Input(shape=(max_length,))\n",
    "    input_exog = Input(shape=(x_train_num.shape[1], ))\n",
    "    input_tfidf = Input(shape=(x_train_tfidf.shape[1], ))\n",
    "\n",
    "    embedding_layer = Embedding(input_dim=num_words,\n",
    "                                output_dim=embedding_dim,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_length,\n",
    "                                trainable=False)(input_ts)\n",
    "\n",
    "    pooled_output = Flatten()(embedding_layer)\n",
    "    layer_1 = Dense(256, kernel_regularizer = tf.keras.regularizers.l2(0.01), activation = \"relu\")(pooled_output)\n",
    "    dropout_1 = Dropout(0.2)(layer_1)\n",
    "    layer_2 = Dense(128, activation = \"relu\")(dropout_1)\n",
    "\n",
    "    layer_num1 = Dense(256, kernel_regularizer = tf.keras.regularizers.l2(0.01), activation = \"relu\")(input_exog)\n",
    "    dropout_2 = Dropout(0.2)(layer_num1)\n",
    "    layer_num2 = Dense(128, activation = \"relu\")(dropout_2)\n",
    "\n",
    "    layer_tfidf1 = Dense(256, kernel_regularizer = tf.keras.regularizers.l2(0.01), activation = \"relu\")(input_tfidf)\n",
    "    dropout_3 = Dropout(0.2)(layer_tfidf1)\n",
    "    layer_tfidf2 = Dense(128, activation = \"relu\")(dropout_3)\n",
    "\n",
    "    concat_ft = Concatenate()([layer_2, layer_num2, layer_tfidf2])\n",
    "\n",
    "    final_dense = Dense(64, activation=\"relu\")(concat_ft)\n",
    "    prediction = Dense(3, activation = \"softmax\")(final_dense)\n",
    "    model = Model(inputs=[input_ts, input_exog, input_tfidf], outputs=prediction)\n",
    "    lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda x: 1e-4 * math.exp(-0.1 * x))\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                    optimizer=\"adam\",\n",
    "                    metrics=[\"accuracy\"])\n",
    "\n",
    "    history = model.fit([x_train_pad, x_train_num, x_train_tfidf], y_train,\n",
    "                          epochs=20,\n",
    "                          validation_data=([x_valid_pad, x_valid_num, x_valid_tfidf], y_valid),\n",
    "                          batch_size=64,\n",
    "                          verbose=1,\n",
    "                          callbacks=[lr_scheduler, early_stopping])\n",
    "\n",
    "    # Save the trained model\n",
    "    model.save('mlp_word2vec_tfidf.keras')\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 262ms/step - accuracy: 0.4057 - loss: 4.7365 - val_accuracy: 0.6519 - val_loss: 1.0840 - learning_rate: 1.0000e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 260ms/step - accuracy: 0.6513 - loss: 1.0833 - val_accuracy: 0.6832 - val_loss: 0.9577 - learning_rate: 9.0484e-05\n",
      "Epoch 3/20\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 258ms/step - accuracy: 0.7173 - loss: 0.8972 - val_accuracy: 0.7039 - val_loss: 0.9023 - learning_rate: 8.1873e-05\n",
      "Epoch 4/20\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 260ms/step - accuracy: 0.7596 - loss: 0.7899 - val_accuracy: 0.7013 - val_loss: 0.8786 - learning_rate: 7.4082e-05\n",
      "Epoch 5/20\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 259ms/step - accuracy: 0.7933 - loss: 0.7112 - val_accuracy: 0.7075 - val_loss: 0.8674 - learning_rate: 6.7032e-05\n",
      "Epoch 6/20\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 259ms/step - accuracy: 0.8187 - loss: 0.6472 - val_accuracy: 0.7053 - val_loss: 0.8938 - learning_rate: 6.0653e-05\n",
      "Epoch 7/20\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 261ms/step - accuracy: 0.8409 - loss: 0.5968 - val_accuracy: 0.7035 - val_loss: 0.8854 - learning_rate: 5.4881e-05\n",
      "Epoch 8/20\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 264ms/step - accuracy: 0.8695 - loss: 0.5479 - val_accuracy: 0.6930 - val_loss: 0.9225 - learning_rate: 4.9659e-05\n",
      "Epoch 9/20\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 263ms/step - accuracy: 0.8860 - loss: 0.5031 - val_accuracy: 0.6908 - val_loss: 0.9213 - learning_rate: 4.4933e-05\n",
      "Epoch 10/20\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 260ms/step - accuracy: 0.9120 - loss: 0.4523 - val_accuracy: 0.6861 - val_loss: 0.9492 - learning_rate: 4.0657e-05\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "x_train_tfidf = vectorizer.fit_transform(x_train[\"cleaned_text\"])\n",
    "x_valid_tfidf = vectorizer.transform(x_valid[\"cleaned_text\"])\n",
    "x_test_tfidf = vectorizer.transform(x_test[\"cleaned_text\"])\n",
    "model, history = train_mlp_word2vec_tfidf(x_train_pad, x_valid_pad, x_train_num, x_valid_num, x_train_tfidf, x_valid_tfidf, y_train_cat, y_valid_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.63      0.69      1001\n",
      "           1       0.64      0.76      0.69      1430\n",
      "           2       0.78      0.71      0.74      1103\n",
      "\n",
      "    accuracy                           0.71      3534\n",
      "   macro avg       0.73      0.70      0.71      3534\n",
      "weighted avg       0.72      0.71      0.71      3534\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict([x_test_pad, x_test_num, x_test_tfidf])\n",
    "preds = np.argmax(preds, axis=1)\n",
    "label = np.argmax(y_test_cat, axis=1)\n",
    "print(classification_report(label, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROBERTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Egen\\Kuliah\\Discover NUS\\Natural Language Processing\\venv\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From d:\\Egen\\Kuliah\\Discover NUS\\Natural Language Processing\\venv\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'roberta.embeddings.position_ids', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
    "roberta_model = TFRobertaModel.from_pretrained('roberta-base')\n",
    "roberta_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roberta_embeddings(texts, max_length=128, batch_size=32):\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i : i + batch_size]\n",
    "        \n",
    "        tokens = tokenizer(batch_texts, padding=\"max_length\", truncation=True,\n",
    "                           max_length=max_length, return_tensors=\"tf\")\n",
    "        input_ids, attention_mask = tokens[\"input_ids\"], tokens[\"attention_mask\"]\n",
    "\n",
    "        roberta_output = roberta_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        batch_embeddings = roberta_output.last_hidden_state[:, 0, :] \n",
    "\n",
    "        all_embeddings.append(batch_embeddings.numpy())\n",
    "    \n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "def train_mlp_roberta(x_train_embed, x_valid_embed, x_train_num, x_valid_num, y_train, y_valid):\n",
    "    input_roberta = Input(shape=(x_train_embed.shape[1],))\n",
    "    input_exog = Input(shape=(x_train_num.shape[1], ))\n",
    "\n",
    "    layer_1 = Dense(256, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.01))(input_roberta)\n",
    "    dropout_1 = Dropout(0.2)(layer_1)\n",
    "    layer_2 = Dense(128, activation=\"relu\")(dropout_1)\n",
    "\n",
    "    layer_num1 = Dense(256, kernel_regularizer=tf.keras.regularizers.l2(0.01), activation=\"relu\")(input_exog)\n",
    "    dropout_2 = Dropout(0.2)(layer_num1)\n",
    "    layer_num2 = Dense(128, activation=\"relu\")(dropout_2)\n",
    "\n",
    "    concat_ft = Concatenate()([layer_2, layer_num2])\n",
    "    prediction = Dense(3, activation=\"softmax\")(concat_ft)\n",
    "\n",
    "    model = Model(inputs=[input_roberta, input_exog], outputs=prediction)\n",
    "\n",
    "    lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda x: 1e-4 * math.exp(-0.1 * x))\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=\"adam\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "    history = model.fit([x_train_embed, x_train_num], y_train,\n",
    "                        epochs=100,\n",
    "                        validation_data=([x_valid_embed, x_valid_num], y_valid),\n",
    "                        batch_size=64,\n",
    "                        verbose=1,\n",
    "                        callbacks=[lr_scheduler, early_stopping])\n",
    "\n",
    "    model.save('mlp_roberta.keras')\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_text = x_train[\"cleaned_text\"].astype(str).tolist()\n",
    "x_valid_text = x_valid[\"cleaned_text\"].astype(str).tolist()\n",
    "x_test_text = x_test[\"cleaned_text\"].astype(str).tolist()\n",
    "x_train_embed = get_roberta_embeddings(x_train_text)\n",
    "x_valid_embed = get_roberta_embeddings(x_valid_text)\n",
    "x_test_embed = get_roberta_embeddings(x_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.3478 - loss: 4.8852 - val_accuracy: 0.4303 - val_loss: 2.0723 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.4000 - loss: 1.9797 - val_accuracy: 0.4969 - val_loss: 1.3230 - learning_rate: 9.0484e-05\n",
      "Epoch 3/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.4187 - loss: 1.3675 - val_accuracy: 0.4631 - val_loss: 1.1472 - learning_rate: 8.1873e-05\n",
      "Epoch 4/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.4578 - loss: 1.1916 - val_accuracy: 0.5013 - val_loss: 1.0753 - learning_rate: 7.4082e-05\n",
      "Epoch 5/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.4940 - loss: 1.1155 - val_accuracy: 0.5362 - val_loss: 1.0468 - learning_rate: 6.7032e-05\n",
      "Epoch 6/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5262 - loss: 1.0623 - val_accuracy: 0.5995 - val_loss: 0.9901 - learning_rate: 6.0653e-05\n",
      "Epoch 7/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5548 - loss: 1.0181 - val_accuracy: 0.5609 - val_loss: 0.9675 - learning_rate: 5.4881e-05\n",
      "Epoch 8/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5757 - loss: 0.9892 - val_accuracy: 0.6330 - val_loss: 0.9340 - learning_rate: 4.9659e-05\n",
      "Epoch 9/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5859 - loss: 0.9689 - val_accuracy: 0.6140 - val_loss: 0.9199 - learning_rate: 4.4933e-05\n",
      "Epoch 10/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6029 - loss: 0.9408 - val_accuracy: 0.6388 - val_loss: 0.9040 - learning_rate: 4.0657e-05\n",
      "Epoch 11/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6184 - loss: 0.9308 - val_accuracy: 0.6450 - val_loss: 0.8934 - learning_rate: 3.6788e-05\n",
      "Epoch 12/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6216 - loss: 0.9141 - val_accuracy: 0.6195 - val_loss: 0.9014 - learning_rate: 3.3287e-05\n",
      "Epoch 13/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6283 - loss: 0.9013 - val_accuracy: 0.6410 - val_loss: 0.8795 - learning_rate: 3.0119e-05\n",
      "Epoch 14/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6282 - loss: 0.8955 - val_accuracy: 0.6642 - val_loss: 0.8674 - learning_rate: 2.7253e-05\n",
      "Epoch 15/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6375 - loss: 0.8832 - val_accuracy: 0.6551 - val_loss: 0.8627 - learning_rate: 2.4660e-05\n",
      "Epoch 16/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6474 - loss: 0.8776 - val_accuracy: 0.6704 - val_loss: 0.8560 - learning_rate: 2.2313e-05\n",
      "Epoch 17/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6428 - loss: 0.8753 - val_accuracy: 0.6737 - val_loss: 0.8497 - learning_rate: 2.0190e-05\n",
      "Epoch 18/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6489 - loss: 0.8667 - val_accuracy: 0.6733 - val_loss: 0.8485 - learning_rate: 1.8268e-05\n",
      "Epoch 19/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6425 - loss: 0.8736 - val_accuracy: 0.6759 - val_loss: 0.8425 - learning_rate: 1.6530e-05\n",
      "Epoch 20/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6518 - loss: 0.8567 - val_accuracy: 0.6632 - val_loss: 0.8453 - learning_rate: 1.4957e-05\n",
      "Epoch 21/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6424 - loss: 0.8595 - val_accuracy: 0.6781 - val_loss: 0.8375 - learning_rate: 1.3534e-05\n",
      "Epoch 22/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6489 - loss: 0.8560 - val_accuracy: 0.6784 - val_loss: 0.8359 - learning_rate: 1.2246e-05\n",
      "Epoch 23/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6515 - loss: 0.8524 - val_accuracy: 0.6777 - val_loss: 0.8332 - learning_rate: 1.1080e-05\n",
      "Epoch 24/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6558 - loss: 0.8473 - val_accuracy: 0.6792 - val_loss: 0.8322 - learning_rate: 1.0026e-05\n",
      "Epoch 25/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6560 - loss: 0.8545 - val_accuracy: 0.6781 - val_loss: 0.8301 - learning_rate: 9.0718e-06\n",
      "Epoch 26/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6625 - loss: 0.8424 - val_accuracy: 0.6762 - val_loss: 0.8287 - learning_rate: 8.2085e-06\n",
      "Epoch 27/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6551 - loss: 0.8455 - val_accuracy: 0.6802 - val_loss: 0.8265 - learning_rate: 7.4274e-06\n",
      "Epoch 28/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6534 - loss: 0.8488 - val_accuracy: 0.6762 - val_loss: 0.8280 - learning_rate: 6.7206e-06\n",
      "Epoch 29/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6562 - loss: 0.8460 - val_accuracy: 0.6799 - val_loss: 0.8258 - learning_rate: 6.0810e-06\n",
      "Epoch 30/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6591 - loss: 0.8420 - val_accuracy: 0.6795 - val_loss: 0.8247 - learning_rate: 5.5023e-06\n",
      "Epoch 31/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6569 - loss: 0.8454 - val_accuracy: 0.6828 - val_loss: 0.8224 - learning_rate: 4.9787e-06\n",
      "Epoch 32/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6587 - loss: 0.8427 - val_accuracy: 0.6799 - val_loss: 0.8230 - learning_rate: 4.5049e-06\n",
      "Epoch 33/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6572 - loss: 0.8437 - val_accuracy: 0.6802 - val_loss: 0.8212 - learning_rate: 4.0762e-06\n",
      "Epoch 34/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6635 - loss: 0.8332 - val_accuracy: 0.6806 - val_loss: 0.8204 - learning_rate: 3.6883e-06\n",
      "Epoch 35/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6596 - loss: 0.8396 - val_accuracy: 0.6802 - val_loss: 0.8205 - learning_rate: 3.3373e-06\n",
      "Epoch 36/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6563 - loss: 0.8391 - val_accuracy: 0.6832 - val_loss: 0.8190 - learning_rate: 3.0197e-06\n",
      "Epoch 37/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6555 - loss: 0.8383 - val_accuracy: 0.6821 - val_loss: 0.8194 - learning_rate: 2.7324e-06\n",
      "Epoch 38/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6603 - loss: 0.8365 - val_accuracy: 0.6817 - val_loss: 0.8183 - learning_rate: 2.4724e-06\n",
      "Epoch 39/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6620 - loss: 0.8304 - val_accuracy: 0.6795 - val_loss: 0.8180 - learning_rate: 2.2371e-06\n",
      "Epoch 40/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6601 - loss: 0.8356 - val_accuracy: 0.6810 - val_loss: 0.8184 - learning_rate: 2.0242e-06\n",
      "Epoch 41/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6603 - loss: 0.8348 - val_accuracy: 0.6810 - val_loss: 0.8180 - learning_rate: 1.8316e-06\n"
     ]
    }
   ],
   "source": [
    "model, history = train_mlp_roberta(x_train_embed, x_valid_embed, x_train_num, x_valid_num, y_train_cat, y_valid_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.42      0.55      1001\n",
      "           1       0.55      0.83      0.66      1430\n",
      "           2       0.82      0.60      0.69      1103\n",
      "\n",
      "    accuracy                           0.64      3534\n",
      "   macro avg       0.71      0.62      0.63      3534\n",
      "weighted avg       0.70      0.64      0.64      3534\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict([x_test_embed, x_test_num])\n",
    "preds = np.argmax(preds, axis=1)\n",
    "label = np.argmax(y_test_cat, axis=1)\n",
    "print(classification_report(label, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roberta_embeddings_3ndim(texts, max_length=128, batch_size=32):\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i : i + batch_size]\n",
    "        \n",
    "        tokens = tokenizer(batch_texts, padding=\"max_length\", truncation=True,\n",
    "                           max_length=max_length, return_tensors=\"tf\")\n",
    "        input_ids, attention_mask = tokens[\"input_ids\"], tokens[\"attention_mask\"]\n",
    "\n",
    "        roberta_output = roberta_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        batch_embeddings = roberta_output.last_hidden_state\n",
    "\n",
    "        all_embeddings.append(batch_embeddings.numpy())\n",
    "    \n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "def train_lstm_roberta(x_train_embed, x_valid_embed, x_train_num, x_valid_num, y_train, y_valid):\n",
    "    input_roberta = Input(shape=(x_train_embed.shape[1], x_train_embed.shape[2]))\n",
    "    input_exog = Input(shape=(x_train_num.shape[1], ))\n",
    "\n",
    "    lstm_1 = LSTM(256, return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(0.01))(input_roberta)\n",
    "    dropout_1 = Dropout(0.2)(lstm_1)\n",
    "    lstm_2 = LSTM(128, return_sequences=False)(dropout_1)\n",
    "\n",
    "    layer_num1 = Dense(256, kernel_regularizer=tf.keras.regularizers.l2(0.01), activation=\"relu\")(input_exog)\n",
    "    dropout_2 = Dropout(0.2)(layer_num1)\n",
    "    layer_num2 = Dense(128, activation=\"relu\")(dropout_2)\n",
    "\n",
    "    concat_ft = Concatenate()([lstm_2, layer_num2])\n",
    "    prediction = Dense(3, activation=\"softmax\")(concat_ft)\n",
    "\n",
    "    model = Model(inputs=[input_roberta, input_exog], outputs=prediction)\n",
    "\n",
    "    lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda x: 1e-4 * math.exp(-0.1 * x))\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=\"adam\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "    history = model.fit([x_train_embed, x_train_num], y_train,\n",
    "                        epochs=10,\n",
    "                        validation_data=([x_valid_embed, x_valid_num], y_valid),\n",
    "                        batch_size=64,\n",
    "                        verbose=1,\n",
    "                        callbacks=[lr_scheduler, early_stopping])\n",
    "\n",
    "    model.save('lstm_roberta.keras')\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_embed_3ndim = get_roberta_embeddings_3ndim(x_train_text)\n",
    "x_valid_embed_3ndim = get_roberta_embeddings_3ndim(x_valid_text)\n",
    "x_test_embed_3ndim = get_roberta_embeddings_3ndim(x_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 431ms/step - accuracy: 0.3905 - loss: 7.9416 - val_accuracy: 0.5828 - val_loss: 2.4863 - learning_rate: 1.0000e-04\n",
      "Epoch 2/10\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 415ms/step - accuracy: 0.5688 - loss: 2.2015 - val_accuracy: 0.6421 - val_loss: 1.3728 - learning_rate: 9.0484e-05\n",
      "Epoch 3/10\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 413ms/step - accuracy: 0.6249 - loss: 1.3776 - val_accuracy: 0.6104 - val_loss: 1.2424 - learning_rate: 8.1873e-05\n",
      "Epoch 4/10\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 414ms/step - accuracy: 0.6327 - loss: 1.1936 - val_accuracy: 0.6501 - val_loss: 1.0791 - learning_rate: 7.4082e-05\n",
      "Epoch 5/10\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 404ms/step - accuracy: 0.6564 - loss: 1.0762 - val_accuracy: 0.6719 - val_loss: 0.9970 - learning_rate: 6.7032e-05\n",
      "Epoch 6/10\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 382ms/step - accuracy: 0.6679 - loss: 1.0139 - val_accuracy: 0.6762 - val_loss: 0.9635 - learning_rate: 6.0653e-05\n",
      "Epoch 7/10\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 386ms/step - accuracy: 0.6785 - loss: 0.9666 - val_accuracy: 0.6730 - val_loss: 0.9547 - learning_rate: 5.4881e-05\n",
      "Epoch 8/10\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 387ms/step - accuracy: 0.6829 - loss: 0.9396 - val_accuracy: 0.6602 - val_loss: 0.9565 - learning_rate: 4.9659e-05\n",
      "Epoch 9/10\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 380ms/step - accuracy: 0.6856 - loss: 0.9146 - val_accuracy: 0.6792 - val_loss: 0.9069 - learning_rate: 4.4933e-05\n",
      "Epoch 10/10\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 404ms/step - accuracy: 0.6910 - loss: 0.8858 - val_accuracy: 0.6959 - val_loss: 0.8820 - learning_rate: 4.0657e-05\n"
     ]
    }
   ],
   "source": [
    "model, history = train_lstm_roberta(x_train_embed_3ndim, x_valid_embed_3ndim, x_train_num, x_valid_num, y_train_cat, y_valid_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 102ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.54      0.62      1001\n",
      "           1       0.60      0.77      0.67      1430\n",
      "           2       0.77      0.67      0.71      1103\n",
      "\n",
      "    accuracy                           0.67      3534\n",
      "   macro avg       0.70      0.66      0.67      3534\n",
      "weighted avg       0.69      0.67      0.67      3534\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict([x_test_embed_3ndim, x_test_num])\n",
    "preds = np.argmax(preds, axis=1)\n",
    "label = np.argmax(y_test_cat, axis=1)\n",
    "print(classification_report(label, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google-001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "x_train_tfidf = vectorizer.fit_transform(x_train[\"cleaned_text\"])\n",
    "x_valid_tfidf = vectorizer.transform(x_valid[\"cleaned_text\"])\n",
    "x_test_tfidf = vectorizer.transform(x_test[\"cleaned_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embed = pd.read_csv(\"train_df_embed.csv\")\n",
    "g001_embed = df_embed[\"Embeddings\"].values\n",
    "x_001 = x.assign(g001_embed = g001_embed)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_001, y, stratify=y, random_state=42, test_size=0.1, shuffle=True)\n",
    "x_train_g001 = np.array([ast.literal_eval(item) for item in x_train[\"g001_embed\"]])\n",
    "x_valid_g001 = np.array([ast.literal_eval(item) for item in x_valid[\"g001_embed\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp_g001(x_train_embed, x_valid_embed, x_train_num, x_valid_num, y_train, y_valid):\n",
    "    input_g001 = Input(shape=(x_train_embed.shape[1],))\n",
    "    input_exog = Input(shape=(x_train_num.shape[1], ))\n",
    "\n",
    "    layer_1 = Dense(256, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.01))(input_g001)\n",
    "    dropout_1 = Dropout(0.2)(layer_1)\n",
    "    layer_2 = Dense(128, activation=\"relu\")(dropout_1)\n",
    "\n",
    "    layer_num1 = Dense(256, kernel_regularizer=tf.keras.regularizers.l2(0.01), activation=\"relu\")(input_exog)\n",
    "    dropout_2 = Dropout(0.2)(layer_num1)\n",
    "    layer_num2 = Dense(128, activation=\"relu\")(dropout_2)\n",
    "\n",
    "    concat_ft = Concatenate()([layer_2, layer_num2])\n",
    "    prediction = Dense(3, activation=\"softmax\")(concat_ft)\n",
    "\n",
    "    model = Model(inputs=[input_g001, input_exog], outputs=prediction)\n",
    "\n",
    "    lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda x: 1e-4 * math.exp(-0.1 * x))\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=\"adam\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "    history = model.fit([x_train_embed, x_train_num], y_train,\n",
    "                        epochs=100,\n",
    "                        validation_data=([x_valid_embed, x_valid_num], y_valid),\n",
    "                        batch_size=64,\n",
    "                        verbose=1,\n",
    "                        callbacks=[lr_scheduler, early_stopping])\n",
    "\n",
    "    model.save('mlp_g001.keras')\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.3584 - loss: 5.1078 - val_accuracy: 0.4314 - val_loss: 2.0556 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.5058 - loss: 1.8358 - val_accuracy: 0.6839 - val_loss: 1.0947 - learning_rate: 9.0484e-05\n",
      "Epoch 3/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.6456 - loss: 1.1172 - val_accuracy: 0.6952 - val_loss: 0.8983 - learning_rate: 8.1873e-05\n",
      "Epoch 4/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.6694 - loss: 0.9313 - val_accuracy: 0.7166 - val_loss: 0.8070 - learning_rate: 7.4082e-05\n",
      "Epoch 5/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.6959 - loss: 0.8474 - val_accuracy: 0.7166 - val_loss: 0.7680 - learning_rate: 6.7032e-05\n",
      "Epoch 6/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.6976 - loss: 0.8098 - val_accuracy: 0.7246 - val_loss: 0.7436 - learning_rate: 6.0653e-05\n",
      "Epoch 7/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.7027 - loss: 0.7855 - val_accuracy: 0.7312 - val_loss: 0.7227 - learning_rate: 5.4881e-05\n",
      "Epoch 8/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.7106 - loss: 0.7563 - val_accuracy: 0.7290 - val_loss: 0.7182 - learning_rate: 4.9659e-05\n",
      "Epoch 9/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.7169 - loss: 0.7499 - val_accuracy: 0.7195 - val_loss: 0.7129 - learning_rate: 4.4933e-05\n",
      "Epoch 10/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.7179 - loss: 0.7301 - val_accuracy: 0.7177 - val_loss: 0.7204 - learning_rate: 4.0657e-05\n",
      "Epoch 11/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.7161 - loss: 0.7352 - val_accuracy: 0.7308 - val_loss: 0.6920 - learning_rate: 3.6788e-05\n",
      "Epoch 12/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.7243 - loss: 0.7192 - val_accuracy: 0.7334 - val_loss: 0.6920 - learning_rate: 3.3287e-05\n",
      "Epoch 13/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.7221 - loss: 0.7088 - val_accuracy: 0.7294 - val_loss: 0.6807 - learning_rate: 3.0119e-05\n",
      "Epoch 14/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.7281 - loss: 0.7027 - val_accuracy: 0.7359 - val_loss: 0.6759 - learning_rate: 2.7253e-05\n",
      "Epoch 15/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.7277 - loss: 0.6882 - val_accuracy: 0.7308 - val_loss: 0.6808 - learning_rate: 2.4660e-05\n",
      "Epoch 16/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.7302 - loss: 0.6913 - val_accuracy: 0.7363 - val_loss: 0.6708 - learning_rate: 2.2313e-05\n",
      "Epoch 17/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.7346 - loss: 0.6875 - val_accuracy: 0.7341 - val_loss: 0.6674 - learning_rate: 2.0190e-05\n",
      "Epoch 18/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.7298 - loss: 0.6804 - val_accuracy: 0.7403 - val_loss: 0.6699 - learning_rate: 1.8268e-05\n",
      "Epoch 19/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.7335 - loss: 0.6790 - val_accuracy: 0.7330 - val_loss: 0.6641 - learning_rate: 1.6530e-05\n",
      "Epoch 20/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.7296 - loss: 0.6843 - val_accuracy: 0.7377 - val_loss: 0.6626 - learning_rate: 1.4957e-05\n",
      "Epoch 21/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.7291 - loss: 0.6793 - val_accuracy: 0.7374 - val_loss: 0.6670 - learning_rate: 1.3534e-05\n",
      "Epoch 22/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.7309 - loss: 0.6847 - val_accuracy: 0.7381 - val_loss: 0.6592 - learning_rate: 1.2246e-05\n",
      "Epoch 23/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.7386 - loss: 0.6713 - val_accuracy: 0.7334 - val_loss: 0.6592 - learning_rate: 1.1080e-05\n"
     ]
    }
   ],
   "source": [
    "model, history = train_mlp_g001(x_train_g001, x_valid_g001, x_train_num, x_valid_num, y_train_cat, y_valid_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp_g001_tfidf(x_train_embed, x_valid_embed, x_train_num, x_valid_num, x_train_tfidf, x_valid_tfidf, y_train, y_valid):\n",
    "    input_g001 = Input(shape=(x_train_embed.shape[1],))\n",
    "    input_exog = Input(shape=(x_train_num.shape[1], ))\n",
    "    input_tfidf = Input(shape=(x_train_tfidf.shape[1], ))\n",
    "\n",
    "    layer_1 = Dense(256, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.01))(input_g001)\n",
    "    dropout_1 = Dropout(0.2)(layer_1)\n",
    "    layer_2 = Dense(128, activation=\"relu\")(dropout_1)\n",
    "\n",
    "    layer_num1 = Dense(256, kernel_regularizer=tf.keras.regularizers.l2(0.01), activation=\"relu\")(input_exog)\n",
    "    dropout_2 = Dropout(0.2)(layer_num1)\n",
    "    layer_num2 = Dense(128, activation=\"relu\")(dropout_2)\n",
    "\n",
    "    layer_tfidf1 = Dense(256, activation=\"relu\")(input_tfidf)\n",
    "    dropout_3 = Dropout(0.2)(layer_tfidf1)\n",
    "    layer_tfidf2 = Dense(128, activation=\"relu\")(dropout_3)\n",
    "\n",
    "    concat_ft = Concatenate()([layer_2, layer_num2, layer_tfidf2])\n",
    "    last_dense = Dense(64, activation=\"relu\")(concat_ft)\n",
    "    prediction = Dense(3, activation=\"softmax\")(last_dense)\n",
    "\n",
    "    model = Model(inputs=[input_g001, input_exog, input_tfidf], outputs=prediction)\n",
    "\n",
    "    lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda x: 1e-4 * math.exp(-0.1 * x))\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=\"adam\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "    history = model.fit([x_train_embed, x_train_num, x_train_tfidf], y_train,\n",
    "                        epochs=100,\n",
    "                        validation_data=([x_valid_embed, x_valid_num, x_valid_tfidf], y_valid),\n",
    "                        batch_size=64,\n",
    "                        verbose=1,\n",
    "                        callbacks=[lr_scheduler, early_stopping])\n",
    "\n",
    "    model.save('mlp_g001_tfidf.keras')\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 60ms/step - accuracy: 0.3662 - loss: 4.4000 - val_accuracy: 0.6399 - val_loss: 1.8161 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 57ms/step - accuracy: 0.6365 - loss: 1.5237 - val_accuracy: 0.7359 - val_loss: 0.9144 - learning_rate: 9.0484e-05\n",
      "Epoch 3/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 54ms/step - accuracy: 0.7647 - loss: 0.8097 - val_accuracy: 0.7454 - val_loss: 0.7429 - learning_rate: 8.1873e-05\n",
      "Epoch 4/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 54ms/step - accuracy: 0.8184 - loss: 0.5914 - val_accuracy: 0.7290 - val_loss: 0.7234 - learning_rate: 7.4082e-05\n",
      "Epoch 5/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 55ms/step - accuracy: 0.8522 - loss: 0.4840 - val_accuracy: 0.7297 - val_loss: 0.7169 - learning_rate: 6.7032e-05\n",
      "Epoch 6/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 56ms/step - accuracy: 0.8690 - loss: 0.4284 - val_accuracy: 0.7254 - val_loss: 0.7371 - learning_rate: 6.0653e-05\n",
      "Epoch 7/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 56ms/step - accuracy: 0.8918 - loss: 0.3768 - val_accuracy: 0.7199 - val_loss: 0.7578 - learning_rate: 5.4881e-05\n",
      "Epoch 8/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 55ms/step - accuracy: 0.9009 - loss: 0.3325 - val_accuracy: 0.7108 - val_loss: 0.7923 - learning_rate: 4.9659e-05\n"
     ]
    }
   ],
   "source": [
    "model, history = train_mlp_g001_tfidf(x_train_g001, x_valid_g001, x_train_num, x_valid_num, x_train_tfidf, x_valid_tfidf, y_train_cat, y_valid_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XLNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at xlnet-base-cased were not used when initializing TFXLNetModel: ['lm_loss']\n",
      "- This IS expected if you are initializing TFXLNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFXLNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFXLNetModel were initialized from the model checkpoint at xlnet-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLNetModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "xlnet_model = TFXLNetModel.from_pretrained(\"xlnet-base-cased\")\n",
    "xlnet_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xlnet_embeddings(texts, batch_size=32, max_length=128):\n",
    "    all_embeddings = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating XLNet Embeddings\"):\n",
    "        batch_texts = texts[i : i + batch_size]\n",
    "        tokens = tokenizer(batch_texts, padding='max_length', truncation=True, max_length=max_length, return_tensors='tf')\n",
    "        \n",
    "        input_ids = tokens[\"input_ids\"]\n",
    "        attention_mask = tokens[\"attention_mask\"]\n",
    "        xlnet_output = xlnet_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        batch_embeddings = tf.reduce_mean(xlnet_output.last_hidden_state, axis=1)\n",
    "\n",
    "        all_embeddings.append(batch_embeddings.numpy())\n",
    "\n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "def train_mlp_xlnet(x_train_embed, x_valid_embed, x_train_num, x_valid_num, y_train, y_valid):\n",
    "    input_xlnet = Input(shape=(x_train_embed.shape[1],))\n",
    "    input_exog = Input(shape=(x_train_num.shape[1], ))\n",
    "\n",
    "    layer_1 = Dense(256, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.01))(input_xlnet)\n",
    "    dropout_1 = Dropout(0.2)(layer_1)\n",
    "    layer_2 = Dense(128, activation=\"relu\")(dropout_1)\n",
    "\n",
    "    layer_num1 = Dense(256, kernel_regularizer=tf.keras.regularizers.l2(0.01), activation=\"relu\")(input_exog)\n",
    "    dropout_2 = Dropout(0.2)(layer_num1)\n",
    "    layer_num2 = Dense(128, activation=\"relu\")(dropout_2)\n",
    "\n",
    "    concat_ft = Concatenate()([layer_2, layer_num2])\n",
    "    prediction = Dense(3, activation=\"softmax\")(concat_ft)\n",
    "\n",
    "    model = Model(inputs=[input_xlnet, input_exog], outputs=prediction)\n",
    "\n",
    "    lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda x: 1e-4 * math.exp(-0.1 * x))\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=\"adam\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "    history = model.fit([x_train_embed, x_train_num], y_train,\n",
    "                        epochs=100,\n",
    "                        validation_data=([x_valid_embed, x_valid_num], y_valid),\n",
    "                        batch_size=64,\n",
    "                        verbose=1,\n",
    "                        callbacks=[lr_scheduler, early_stopping])\n",
    "\n",
    "    model.save('mlp_xlnet.keras')\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating XLNet Embeddings: 100%|██████████| 773/773 [1:03:37<00:00,  4.94s/it]\n",
      "Generating XLNet Embeddings: 100%|██████████| 86/86 [06:15<00:00,  4.36s/it]\n",
      "Generating XLNet Embeddings: 100%|██████████| 111/111 [08:05<00:00,  4.37s/it]\n"
     ]
    }
   ],
   "source": [
    "x_train_text = x_train[\"cleaned_text\"].astype(str).tolist()\n",
    "x_valid_text = x_valid[\"cleaned_text\"].astype(str).tolist()\n",
    "x_test_text = x_test[\"cleaned_text\"].astype(str).tolist()\n",
    "x_train_xlnet_embed = get_xlnet_embeddings(x_train_text)\n",
    "x_valid_xlnet_embed = get_xlnet_embeddings(x_valid_text)\n",
    "x_test_xlnet_embed = get_xlnet_embeddings(x_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.3739 - loss: 5.3913 - val_accuracy: 0.5173 - val_loss: 3.7111 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4589 - loss: 3.6228 - val_accuracy: 0.5438 - val_loss: 2.8463 - learning_rate: 9.0484e-05\n",
      "Epoch 3/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5028 - loss: 2.7995 - val_accuracy: 0.5489 - val_loss: 2.3628 - learning_rate: 8.1873e-05\n",
      "Epoch 4/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5327 - loss: 2.3210 - val_accuracy: 0.5879 - val_loss: 2.0350 - learning_rate: 7.4082e-05\n",
      "Epoch 5/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5580 - loss: 2.0092 - val_accuracy: 0.5988 - val_loss: 1.8208 - learning_rate: 6.7032e-05\n",
      "Epoch 6/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5649 - loss: 1.8074 - val_accuracy: 0.5973 - val_loss: 1.6686 - learning_rate: 6.0653e-05\n",
      "Epoch 7/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5842 - loss: 1.6462 - val_accuracy: 0.5973 - val_loss: 1.5591 - learning_rate: 5.4881e-05\n",
      "Epoch 8/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5909 - loss: 1.5330 - val_accuracy: 0.6086 - val_loss: 1.4671 - learning_rate: 4.9659e-05\n",
      "Epoch 9/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6140 - loss: 1.4361 - val_accuracy: 0.6119 - val_loss: 1.3972 - learning_rate: 4.4933e-05\n",
      "Epoch 10/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6115 - loss: 1.3776 - val_accuracy: 0.6260 - val_loss: 1.3421 - learning_rate: 4.0657e-05\n",
      "Epoch 11/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6202 - loss: 1.3179 - val_accuracy: 0.6202 - val_loss: 1.3035 - learning_rate: 3.6788e-05\n",
      "Epoch 12/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6286 - loss: 1.2668 - val_accuracy: 0.6253 - val_loss: 1.2655 - learning_rate: 3.3287e-05\n",
      "Epoch 13/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6284 - loss: 1.2365 - val_accuracy: 0.6268 - val_loss: 1.2408 - learning_rate: 3.0119e-05\n",
      "Epoch 14/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6380 - loss: 1.2061 - val_accuracy: 0.6279 - val_loss: 1.2186 - learning_rate: 2.7253e-05\n",
      "Epoch 15/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6349 - loss: 1.1835 - val_accuracy: 0.6166 - val_loss: 1.2093 - learning_rate: 2.4660e-05\n",
      "Epoch 16/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6454 - loss: 1.1563 - val_accuracy: 0.6308 - val_loss: 1.1801 - learning_rate: 2.2313e-05\n",
      "Epoch 17/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6478 - loss: 1.1342 - val_accuracy: 0.6271 - val_loss: 1.1704 - learning_rate: 2.0190e-05\n",
      "Epoch 18/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6508 - loss: 1.1265 - val_accuracy: 0.6286 - val_loss: 1.1571 - learning_rate: 1.8268e-05\n",
      "Epoch 19/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6538 - loss: 1.1041 - val_accuracy: 0.6290 - val_loss: 1.1444 - learning_rate: 1.6530e-05\n",
      "Epoch 20/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6552 - loss: 1.1005 - val_accuracy: 0.6253 - val_loss: 1.1410 - learning_rate: 1.4957e-05\n",
      "Epoch 21/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6607 - loss: 1.0832 - val_accuracy: 0.6304 - val_loss: 1.1318 - learning_rate: 1.3534e-05\n"
     ]
    }
   ],
   "source": [
    "model, history = train_mlp_xlnet(x_train_xlnet_embed, x_valid_xlnet_embed, x_train_num, x_valid_num, y_train_cat, y_valid_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.36      0.47      1001\n",
      "           1       0.54      0.78      0.64      1430\n",
      "           2       0.71      0.61      0.65      1103\n",
      "\n",
      "    accuracy                           0.61      3534\n",
      "   macro avg       0.65      0.58      0.59      3534\n",
      "weighted avg       0.63      0.61      0.60      3534\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict([x_test_xlnet_embed, x_test_num])\n",
    "preds = np.argmax(preds, axis=1)\n",
    "label = np.argmax(y_test_cat, axis=1)\n",
    "print(classification_report(label, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm_xlnet(x_train_embed, x_valid_embed, x_train_num, x_valid_num, y_train, y_valid):\n",
    "    input_xlnet = Input(shape=(x_train_embed.shape[1], x_train_embed.shape[2] ))\n",
    "    input_exog = Input(shape=(x_train_num.shape[1], ))\n",
    "\n",
    "    lstm_1 = LSTM(256, return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(0.01))(input_xlnet)\n",
    "    dropout_1 = Dropout(0.2)(lstm_1)\n",
    "    lstm_2 = LSTM(128, return_sequences=False)(dropout_1)\n",
    "\n",
    "    layer_num1 = Dense(256, kernel_regularizer=tf.keras.regularizers.l2(0.01), activation=\"relu\")(input_exog)\n",
    "    dropout_2 = Dropout(0.2)(layer_num1)\n",
    "    layer_num2 = Dense(128, activation=\"relu\")(dropout_2)\n",
    "\n",
    "    concat_ft = Concatenate()([lstm_2, layer_num2])\n",
    "    last_dense = Dense(64, activation=\"relu\")(concat_ft)\n",
    "    prediction = Dense(3, activation=\"softmax\")(last_dense)\n",
    "\n",
    "    model = Model(inputs=[input_xlnet, input_exog], outputs=prediction)\n",
    "\n",
    "    lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda x: 1e-4 * math.exp(-0.1 * x))\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=\"adam\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "    history = model.fit([x_train_embed, x_train_num], y_train,\n",
    "                        epochs=100,\n",
    "                        validation_data=([x_valid_embed, x_valid_num], y_valid),\n",
    "                        batch_size=64,\n",
    "                        verbose=1,\n",
    "                        callbacks=[lr_scheduler, early_stopping])\n",
    "\n",
    "    model.save('lstm_xlnet.keras')\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 14ms/step - accuracy: 0.3699 - loss: 7.4440 - val_accuracy: 0.4915 - val_loss: 2.2201 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.4646 - loss: 1.8896 - val_accuracy: 0.5558 - val_loss: 1.2080 - learning_rate: 9.0484e-05\n",
      "Epoch 3/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.5411 - loss: 1.1753 - val_accuracy: 0.5908 - val_loss: 1.0572 - learning_rate: 8.1873e-05\n",
      "Epoch 4/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.5679 - loss: 1.0554 - val_accuracy: 0.5726 - val_loss: 1.0272 - learning_rate: 7.4082e-05\n",
      "Epoch 5/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.5789 - loss: 1.0075 - val_accuracy: 0.5602 - val_loss: 1.0328 - learning_rate: 6.7032e-05\n",
      "Epoch 6/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.5837 - loss: 0.9816 - val_accuracy: 0.5919 - val_loss: 0.9769 - learning_rate: 6.0653e-05\n",
      "Epoch 7/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.6044 - loss: 0.9486 - val_accuracy: 0.6039 - val_loss: 0.9673 - learning_rate: 5.4881e-05\n",
      "Epoch 8/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.5951 - loss: 0.9462 - val_accuracy: 0.6093 - val_loss: 0.9533 - learning_rate: 4.9659e-05\n",
      "Epoch 9/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.6074 - loss: 0.9292 - val_accuracy: 0.6115 - val_loss: 0.9455 - learning_rate: 4.4933e-05\n",
      "Epoch 10/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.6023 - loss: 0.9255 - val_accuracy: 0.6053 - val_loss: 0.9454 - learning_rate: 4.0657e-05\n",
      "Epoch 11/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.6078 - loss: 0.9159 - val_accuracy: 0.6155 - val_loss: 0.9351 - learning_rate: 3.6788e-05\n",
      "Epoch 12/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.6113 - loss: 0.9065 - val_accuracy: 0.6089 - val_loss: 0.9351 - learning_rate: 3.3287e-05\n",
      "Epoch 13/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.6231 - loss: 0.8944 - val_accuracy: 0.6039 - val_loss: 0.9374 - learning_rate: 3.0119e-05\n",
      "Epoch 14/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.6241 - loss: 0.8978 - val_accuracy: 0.6184 - val_loss: 0.9254 - learning_rate: 2.7253e-05\n",
      "Epoch 15/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.6245 - loss: 0.8895 - val_accuracy: 0.6126 - val_loss: 0.9250 - learning_rate: 2.4660e-05\n",
      "Epoch 16/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.6275 - loss: 0.8833 - val_accuracy: 0.6188 - val_loss: 0.9192 - learning_rate: 2.2313e-05\n",
      "Epoch 17/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.6317 - loss: 0.8734 - val_accuracy: 0.6170 - val_loss: 0.9187 - learning_rate: 2.0190e-05\n",
      "Epoch 18/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.6376 - loss: 0.8725 - val_accuracy: 0.6206 - val_loss: 0.9134 - learning_rate: 1.8268e-05\n",
      "Epoch 19/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.6213 - loss: 0.8851 - val_accuracy: 0.6170 - val_loss: 0.9134 - learning_rate: 1.6530e-05\n",
      "Epoch 20/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.6300 - loss: 0.8706 - val_accuracy: 0.6202 - val_loss: 0.9102 - learning_rate: 1.4957e-05\n",
      "Epoch 21/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.6301 - loss: 0.8751 - val_accuracy: 0.6191 - val_loss: 0.9108 - learning_rate: 1.3534e-05\n",
      "Epoch 22/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.6383 - loss: 0.8671 - val_accuracy: 0.6206 - val_loss: 0.9073 - learning_rate: 1.2246e-05\n",
      "Epoch 23/100\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.6320 - loss: 0.8642 - val_accuracy: 0.6162 - val_loss: 0.9109 - learning_rate: 1.1080e-05\n"
     ]
    }
   ],
   "source": [
    "model, history = train_lstm_xlnet(tf.expand_dims(x_train_xlnet_embed, axis=1), tf.expand_dims(x_valid_xlnet_embed, axis=1), x_train_num, x_valid_num, y_train_cat, y_valid_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.47      0.55      1001\n",
      "           1       0.57      0.68      0.62      1430\n",
      "           2       0.65      0.66      0.65      1103\n",
      "\n",
      "    accuracy                           0.61      3534\n",
      "   macro avg       0.62      0.60      0.61      3534\n",
      "weighted avg       0.62      0.61      0.61      3534\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict([tf.expand_dims(x_test_xlnet_embed, axis = 1), x_test_num])\n",
    "preds = np.argmax(preds, axis=1)\n",
    "label = np.argmax(y_test_cat, axis=1)\n",
    "print(classification_report(label, preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
